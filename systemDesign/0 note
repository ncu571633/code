How Microservices Work:

In a microservices architecture:
    Each service operates independently but collaborates with others to fulfill user requests.
    Communication occurs through well-defined APIs.
    Services can be hosted on different servers or containers and managed separately

Advantages Over Monolithic Architecture:
    Agility: Faster development and deployment cycles due to smaller, focused units of work.
    Resilience: Failures in one service do not cascade to the entire system.
    Scalability: Only the required services are scaled, rather than the entire application


1. clarify the requirement/goal of the system
    traffic size: daily active user
    
    Functional requirement
    (1) tweet: create, delete
    (2) timeline/feed: home, user
    (3) follow a user
    (4) like a tweet
    (5) search tweet
    
     Non-functional requirement
     (consistency): Consistency means that a read request for a specific data made at any of the nodes of the distributed system database should return the same value.
		Strong consistency:
		The reads are guaranteed to return the most recent committed version of an item. 
		A client never sees an uncommitted or partial write. Users are always guaranteed to read the latest committed write.
		
	    1. Eventual consistency:
		is ideal where the application doesn't require any ordering guarantees. 
		Examples include count of Retweets, Likes, or non-threaded comments.
	        
		every request receive a response, without the guarantee that it contains the most recent write
		data is written to the primary node, then propagated to the read-only secondary nodes. 

	    2. Consistent prefix: client read the data in the same sequence (D, C, B, A) that has been committed (A, B, C, D) 
                顺序不乱。
	    3. Session consistency: committed users will see the data that they just committed.
                保证该用户 session 
	    4. Bounded staleness: indicator to define how much staleness period you set. 
                在 time windows，肯定没有stale data
	    5. Strong consistency may also suffer from reduced availability (during failures) because data can't replicate and commit in every region. 
		Eventual consistency offers higher availability and better performance, but it's more difficult to program applications because data may not be consistent across all regions.Feb 27, 2023


     availability: the system responds quickkly, even during failures.
     scalability: be scaled to support large data set. 
     Low Latency: low latency search, < 500ms
     partition tolerance: system continues to operate despite a number of messages being dropped/delayed.
     Fault Tolerance: How well does the system need to handle failures? Consider redundancy, failover, and recovery mechanisms.
     Durability: How important is it that the data in your system is not lost? For example, a social network might be able to tolerate some data loss, 
but a banking system cannot.
     Security: How secure does the system need to be? Consider data protection, access control, and compliance with regulations.

CAP theorem
	(1) consistency: all clients see the same data at the same time no matter which node they connect to.
	(2) availability: every request will receive a response, even during network partitions. 
The tradeoff is that different nodes may temporarily have different versions of the data, leading to inconsistency. 
The system will eventually reconcile these differences, but there's no guarantee about when this will happen
	(3) partition tolerance: a partition indicates a communication break between two nodes. Partition tolerance means the system continues to operate despite network partitions. 
	
	choose consistency over availability: block all write operations to avoid data inconsistency among DB servers, which makes system unavailable. 
	choose availability over consisntency: system keeps accepting reads, even it might return stale data.

Examples of systems that require strong consistency include:
    Banking systems where the balance of an account must be consistent across all nodes to prevent fraud
    Booking systems for limited resources (airline seats, event tickets, hotel rooms) where you need to prevent double-booking
    Inventory management systems, where stock levels need to be precisely tracked to avoid overselling products

Communication Protocols
1. Internally, for a typical microservice application which consistitues 90%+ of system design problems, either HTTP(S) or gRPC will do the job. 
Don't make things complicated.
2. Externally, you'll need to consider how your clients will communicate with your system

2. capacity estimation。 可以不用，perform calculations only if they will directly influence your design
	how many daily active user, how many tweets
	each user: visit timeline 5 times
	every timeline/page: 20 tweets
	each tweet has size 280 bytes, metadata 30 bytes, photo/video
	  storage estimate: write size daily: text 100M * (280+30) = 31GB/day
	  bandwidth estimate: 200M*(5 home visit) * 20 tweets /page = 32B tweets/day
	          text: 32B*280bytes/86400 = 100MB/S
	daily write size
	replication: redundancy: replication * 3, 
	      

3. system APIs
 gRPC and REST are both API design approaches.
but gRPC prioritizes high-performance, efficient communication, especially for microservices and real-time applications, 
while REST is simpler, more flexible, and widely used for web applications. 

	postTweet(userToken, string tweet)
	deleteTweet(userToken, string tweetID)
	likeOrUnlickTweet(userToken, string tweetId, bool like)
	readHomeTimeLine(userToken, int pageSize, pageToken)
	readUserTimeLine(userToken, int pageSize, pageToken)

How to design a REST API
addBook(), getBook()
            1. endpoint
            2. HTTP Method
            3. RequestBody/PathVariable
            4. ResponseBody
Considering book as a resource, the following can be the REST APIs:

    create a book resouce
    POST /api/books
    get a particular book
    GET /api/books/{id}
    update details of a book
    PUT /api/books/{ID}
    get all books
    GET /api/books
    search books collection
    GET /api/books?name=twinke
    GET /api/books?author=nobody
    delete a particular book
    DELETE /api/books/{id}


[Optional] Data Flow 
the high level sequence of actions or processes that the system performs on the inputs to produce the desired outputs
  For a web crawler, this might look like:
    Fetch seed URLs
    Parse HTML
    Extract URLs
    Store data
    Repeat

4. data storage
	(1) SQL or NoSQL
		table: user table, tweet table, follower table
	Structure:
	SQL databases are table based
	NoSQL databases can be document-oriented, key-value pairs, or graph structures.
	
	Scalability:
		SQL databases scale vertically, usually on a single server
		NoSQL databases offer horizontal scalability, meaning that more servers simply need to be added to increase their data load. 
	This means that NoSQL databases are better for modern cloud-based infrastructures, which offer distributed resources.
	
	When to use NoSQL:
		Low latency;
		unstructureed, have no relation data.
		only need to serialize and deserialize data.
		need to store massive amount of data.
	
		high performance, particularly read performance: The way distributed NoSQL systems like Cassandra and Riak work means you can usually get very high read performance by adding more boxes. 
	Some go so far as to automatically replicate data across nodes to ensure you always have plenty of copies of your data to access.
	    	high availability (HA): Data replicates across nodes in a NoSQL system, so the failure of a single node does not necessarily result in data loss or downtime for your application. 
	This also means you can easily add or remove nodes from clusters without impacting availability.
	
	(2) Sharding:
Why: 
Scalability: horizontal scaling by allowing organizations to distribute data across multiple servers
No Single Server Bottleneck: By spreading data and load, sharding prevents any one server from becoming a performance bottleneck
Performance: Improved Query Response Times: Each shard contains only a subset of the total data, 
         so queries can be executed faster since each server scans fewer rows
      Parallel Processing: With data distributed, multiple servers can process queries simultaneously
Higher Availability: Isolates failures; system remains operational if a shard fails

	Range-based sharding: based on a range of values.
		for example, name first alphabet A to I -> shard A, J to S -> shard B, T to Z: shard C...
	Hashed sharding: hash function(key) -> hash value
	Directory sharding: lookup table
	Geo sharding
	
	(3) Replication
	1 Master DB: supports write operations.
	3 Worker DB: gets copies of the data from the master DB and only supports read operations.
	All the data-modifying commands like inserts, delete or update must be sent to the master DB.
	Most applications require a much higher ratio of reads to writes, the number of worker DB is larger than the number of master DB.
	
	Better performance: allows more queries to be processed in parallel. 
	Reliability: 
	High availability
	
	If one DB goes offline：
	a. if only one worker DB is available and it goes offline, read operation will be directed to the master DB.
	If multiple worker DB are available, read operations are redirected to other healthy workder DB.
	b. if master DB goes offline, a worker DB will be promoted to be the new master(voting algorithm). All DB operations will be temporarily executed on the new master DB. 
	Complicated: since the data in a worker DB may not be up to date. The missing data needs to be updated by running data recovery scripts.

	If the DB goes back online:
	a. Catchup recovery: the worker (which got disconnected) can connect to the leader again and request all data changes that occur when the follower was disconnected.
 

	(4). Multi-Leader replication
	one master in each data center.
	Problem: two different users try to write different masters in different data center, which causes a conflict. 
    		Conflict Detection(too late)
 		Conflict Avoidance: application can ensure that all writes for a particular record go through the same master (data center), then conflicts cannot occur.
	in normal operation, users are geoDNS rounded to the closest data center, with a split traffic of x% in east and 100-x in west.

Index: Indexing is about making data faster to query
1. The most basic method of indexing is simply keeping our data in a hash map by a specific key. 
When we need to grab data by that key, we can do so in O(1) time. 
2. data in a sorted list: do binary search to find the data we need in O(log n) time. This is a common way of indexing data in databases.
3. most relational databases allow you to create indexes on any column or group of columns in a table
4. Geospatial indexes are used to index location data. 
This is useful for systems that need to do things like find the nearest restaurant or the nearest gas station
5. Vector databases are used to index high-dimensional data. 
This is useful for systems that need to do things like find similar images or similar documents

5. Message queue
	supports async communication, servers as a buffer and distributes async requests. 
	producers/consumers.

	advantage:
		decoupling makes the message queue a preferred architecture for building a scalable and reliable application, producer and consumer can be scaled independently.
		server publish processing jobs to the message queue. 


6. high level system design
user -> API Gateway -> load balancer  -> tweet writer server  -> DB
	               	             -> timeline service     -> cache   (need low latency, read cache join together on the fly)

        API Gateway: authentication, rate limiting, and logging

	load balancer: evenly distributs incoming traffic amoung web servers.
            L4 load balancer: Transport Layer (Layer 4) Route traffic based on IP addresses, TCP/UDP ports, and basic network information
                     Generally faster due to less processing overhead
                     Simple traffic distribution, basic health checks, routing based on network characteristics
            L7 load balancer: Application Layer (Layer 7). Inspect application data, including HTTP headers, URLs, cookies, 
                 and content to make routing decisions
                 Content-based routing, SSL termination, session persistence, application-aware load balancing
                 Web applications, APIs, services requiring advanced routing and content manipulation

A service mesh is a dedicated infrastructure layer that manages and controls communication 
between microservices in a distributed application. 
Centralized Traffic Management, Observability and Monitoring, 
Service Discovery and Dynamic Scaling(Automated service discovery allows new services to be registered and found dynamically)

	home timeline: 
	 (1)pull mode: fetch tweets from N followers from DB, merge and return
	     pros: write is fast: O(1)
	     cons: read is slow: O(N) db reads
	 (2) push mode: maintain a feedlist in cache for each user, fanout on write
	      pros: read is fast: O(1) from the feed list in cache
	      cons: write needs more efforts: O(n) write for each new tweets:
	            async tasks
	            delay in showing latest tweets
	 fanout on write:
	    not efficient for user with huge amount of followers
	    hybrid solution: hot user, not hot users


	Stateless architecture: HTTP requests from users can be sent to any web servers, which fetch state data(user session data) from a shared data store. 
		State data is stored in a shared data store, and kept out of web servers. 
		Use NoSQL for session data, as it is easy to scale. 
		A stateful server remembers client data(state) from one request to the next. So every request from th same client must be routed to the same server. 
6. scalability
	identify potential bottleneck
	discussion solutions, tradeoffs:
		data sharding: data store, cache
		loading balancing: between user/server, server/db
		data caching: read heavy
data sharding: break large table into smaller shards on multiple servers
why sharding: 
    Scalability: Sharding allows you to scale horizontally by adding more servers, each handling a subset of the data, 
thus accommodating growth without degrading performance
    Performance: Querying a massive, unsharded database can be slow because the system must scan large tables. 
With sharding, each shard contains fewer rows, so queries are faster 
    Fault Tolerance and High Availability: If a single database server fails, an unsharded system can become completely unavailable

pros: horizontal scaling
    shard by creation time
        pros: limited shards to query
        cons: hot/cold data issue, new shards fill up quickly
    shard by hash(user id):
    	pros: simple, query user timeline is straightforward
    	cons: home timeline still need to query multiple shards
    	      non-uniform distribvution
    	      hot user
    shard by hash(tweetid)
    	pros: uniform distribution, high availability
    	cons: need to query all shards in order to generate user/home timeline
	
	
Elasticsearch: a distributed search engine built on Apache Lucene
	1. indexing
	put
	2. query:
	get
	3. "AND", "OR" are supported.
	4. Ranking: bm25
	estimate the relevance of documents to a given search query


Scale from zero to millions of users

1. Single server setup
User  DNS
1 server(web app, database, cache)

2. Database

User  DNS
1 server(web app, cache)
database server:
Which database to use:
  relational database: SQL
  nonrelational database: unstructured data, low latency, store large data

Vertical scaling vs horizontal scaling
  vertical scaling: add more powers(CPU, RAM)
  horizontal scaling: more servers

Cache:
1. When to use cache: read frequently, not modified infrequently
2. Expiration policy
3. Consistency

Memcached is a general-purpose distributed memory-caching system: cache data from db.

Asynchronous processing: user doesn't need to wait the job to finish, and continue to do the next task.
   for example: upload video/photo
   message queue: kafka
   
Rehashing problem: 
  most keys are redistributed, not just the ones originally stored in the offline server. 

Consisteng hashing: M hashed key space, and N servers. 
A partition is the hash space between adjacent servers.

1. Create a circular number line(from 0 to M-1).
2. Map the server based on IP or name onto the ring. 
3. Move the hash keys in the clockwise direction until the first server was found. 

  virtual nodes: each server is represented by multiple virtual nodes on the ring. 
	Each server is responsible for multiple partitions. 
	As the number of virtual nodes increases, the distribution of keys becomes more balanced. 

RESTful API(Representational state transfer)
* HTTP protocol for data communication.
* REST server contains resource ←access by HTTP methods←REST client get the data
human readable JSON
gRPC uses protocol buffer which reduces file size.
* gRPC: Best for Internal APIs, IoT, and mobile, but slow implementation
* Being cacheable is one of architectural constraints of REST.

    GET requests should be cacheable by default. Usually browsers treat all GET requests cacheable.
    POST requests are not cacheable by default but can be made cacheable by either an Expires header, or a Cache-Control header is added to the response.
    PUT and DELTE are not cacheable at all

                REST	gRPC
Cross-platform	Yes	Yes
Message Format	Custom but generally JSON or XML	Protocol buffers
Message Payload Size	Medium/Large	Small
Processing Complexity	Higher (text parsing)	Lower (well-defined binary structure)
Browser Support	Yes (native)	Yes (via gRPC-Web)

websocket vs SSE(server sent event)
Websocket(bi-direction) and SSE(single direction) are 2 common way we build a connection with backend and keep it live to send/receive data; 


HTTP vs. REST API
REST (Representational State Transfer) API is an architectural style for building web services, typically over HTTP.
HTTP provides standard methods like GET, POST, PUT, DELETE, etc.


Cache and DB consistency
1. Write through: in which we write DB first and then update the cache; 
2. Write back, in which we update cache first, and then at sometime later right back to DB. 
Write back could be used if we decided to in real time update the winning bid into the auction_table to reduce the volume of write request.

It is possible that we write to DB success but failed to update cache. 
For example, the request to update cache is failed or the node is down before try to update the cache. 
Retry could be used here, but it could still possible that the update to cache is failed after several retry. 
But since our Fulfillment Service reads the cache to execute the bid, 
it might read some outdated data because of the above potential failure. 

