non-relational database. 
Key must be unique: text or hashed values.
Value can be string, list, objects.

Functionality requirement
  put(key value)
  get(key)

Non-Functionality requirement
  high availability: responds quickly, even during failures.
  high scalability: can be scaled to support large data set
  consistency.
  low latency

Single server: 
  memory: data compression, store frequently used data in memory and the rest on disk.

Distributed key-value store

CAP theorem: Consistency, availability, partition tolerance
  consistency: all clients see the same data at the same time no matter which node they connect to.
  availability: any client which requests data gets a response even if some of the nodes are down.
  partition tolerance: system continues to operate despite network partitions.

Data partition:
  1. distribute data across multiple servers evenly. 
  2. minimize data movement when nodes are added or removed. 

Consistent hashing:
  1. servers are placed on a hash ring: s0, s1, s2... s7
  2. a key is hashed onto the same ring, and it is stored on the first server encountered when moving in the clockwise direction. 
  Advantage:
  automatic scaling: servers could be added and removed automatically depending on the loads.
  Heterogeneity: the number of virtual nodes for a server is proportional to the server capacity (servers with higher capacity are asigned with more virtual nodes)

Data replication:
  1. data should be replicated asynchronously over N servers, where N is a configurable parameter.
  2. These N servers are chosen: after a key is mapped to a position on the hash ring, walk clockwise from that position and choose first N servers on the ring to store data copies. 
  nodes in the same data center often fail at the same time due to power outages. For better reliability, replicas are placed in distinct data centers. 

Consistency:
  Coordinator: proxy between the client and the nodes.
  N: the nubmer of replicas
  W: write quorum of size W: for a write operation to be considered as successful, write operation must be acknowledged from W replicas.
  (N=3, W=1: the coordinator must receive at least one acknowledgement before the write operation is considered as successful)
  R: read quorum of size R: read operation must wait for responses from at least R replicas.
  Configuration of W/R/N is a tradeoff between latency and consistency. 
  (w=1, r=1) an operation is returned quickly.
  (W>1, r>1) better consistency. but the query will be slower.
  Some examples:
  (W+R > N): strong consistency: at least one overlapping node has the latest data to ensure consistency.
  (R=1, W=N): fast read
  (W=1, R=N): fast write.
  
  client -> coordinator -> s0 -> s1
                              -> s2

Consistency models:
  strong consistency: any read operation returns a value corresponding to the result of the most updated write data.
    forcing a replica not accepts new reads/writes until every replica has agreed on current write.
  weak consistency: 
  eventual consistency: given enough time, all updates are propagated, and all replicas are consistent. 

Inconsistency resolution:
1. Last-Write-Wins (LWW) with Timestamps
    Mechanism: Assign timestamps to updates and prioritize the latest one.
    Limitation: Requires synchronized clocks; clock drift can cause data loss
2. Version Vectors
    Track multiple versions of data across nodes using vector clocks.
   Resolve conflicts by comparing version histories to determine update order
  a versioning system that can detect conflicts and reconcile conflicts.
  a vector clock is a common technique to solve this problem: [server, version] pair

Gossip protocol:
  Each node maintains a node membership list, which contains member IDs and heartbeat counters.
  each node increase the counter.
  Each node periodically sends heartbeats to a set of random nodes.
  Once nodes receive heartbeats, list is updated to the latest info. 
  if the heartbeat has not increased for more than a period of time, the member is consideered as offline. 

Handling permanent failure(keep replicas in sync)
  Merkle tree: inconsistency detection, minimizing the amount of data transferred.
      non-leaf node is labeled with the hash of the labels or values of its child node.
    compare two merkle tree, start by comparing the root hashes. If root hashes match, both servers have the same data.
  By using Merkle trees, the amount of data needed to by synchronized its proportional to the differences between the two replicas, and not the amount of data they contain.

Handling data center outage:
  replicate data across multiple data centers. If one data center is completely offline, users can still access data through other data centers.

Write path:
Client -> commit log
       -> memory cache -> flush to disk SSTables

Read Path:
Client -> memory cache
      Bloom filter (used to figure out which SSTables might contain the key)
      SSTable return the result of data set.
      
