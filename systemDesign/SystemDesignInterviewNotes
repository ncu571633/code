1. clarify the requirement/goal of the system
    traffic size: daily active user
    functionalities, components
    (1) tweet: create, delete
    (2) timeline/feed: home, user
    (3) follow a user
    (4) like a tweet
    (5) search tweet
    
     non-functional requirement
     (consistency): every read receive the most recent write or an error
                    sacrifice: eventual consistency
     availability: every request receive a response, without the guarantee that it contains the most recent write
                   scalable: performance: low latency
     partition tolerance: system continues to operate despite a number of messages being dropped/delayed.                
    
2. capacity estimation
	how many daily active user, how many tweets
	each user: visit timeline 5 times
	every timeline/page: 20 tweets
	each tweet has size 280 bytes, metadata 30 bytes, photo/video
	  storage estimate: write size daily: text 100M * (280+30) = 31GB/day
	  bandwidth estimate: 200M*(5 home visit) * 20 tweets /page = 32B tweets/day
	          text: 32B*280bytes/86400 = 100MB/S
	daily write size
	replication: redundancy: replication * 3, 
	      

3. system APIs
	postTweet(userToken, string tweet)
	deleteTweet(userToken, string tweetID)
	likeOrUnlickTweet(userToken, string tweetId, bool like)
	readHomeTimeLine(userToken, int pageSize, pageToken)
	readUserTimeLine(userToken, int pageSize, pageToken)
		
4. high level system design
	user -> load balancer  -> tweet writer server  -> DB
	                       -> timeline service     -> cache   (need low latency, read cache join together on the fly)
	                       
	home timeline: 
	 (1)pull mode: fetch tweets from N followers from DB, merge and return
	     pros: write is fast: O(1)
	     cons: read is slow: O(N) db reads
	 (2) push mode: maintain a feedlist in cache for each user, fanout on write
	      pros: read is fast: O(1) from the feed list in cache
	      cons: write needs more efforts: O(n) write for each new tweets:
	            async tasks
	            delay in showing latest tweets
	 fanout on write:
	    not efficient for user with huge amount of followers
	    hybrid solution: hot user, not hot users

5. data storage
	table: user table, tweet table, follower table
	
6. scalability
	identify potential bottleneck
	discussion solutions, tradeoffs:
		data sharding: data store, cache
		loading balancing: between user/server, server/db
		data caching: read heavy
data sharding: break large table into smaller shards on multiple servers
pros: horizontal scaling
    shard by creation time
        pros: limited shards to query
        cons: hot/cold data issue, new shards fill up quickly
    shard by hash(user id):
    	pros: simple, query user timeline is straightforward
    	cons: home timeline still need to query multiple shards
    	      non-uniform distribvution
    	      hot user
    shard by hash(tweetid)
    	pros: uniform distribution, high availability
    	cons: need to query all shards in order to generate user/home timeline
	
	






Scale from zero to millions of users

1. Single server setup
User  DNS
1 server(web app, database, cache)

2. Database

User  DNS
1 server(web app, cache)
database server:
Which database to use:
  relational database: SQL
  nonrelational database: unstructured data, low latency, store large data

Vertical scaling vs horizontal scaling
  vertical scaling: add more powers(CPU, RAM)
  horizontal scaling: more servers

Cache:
1. When to use cache: read frequently, not modified infrequently
2. Expiration policy
3. Consistency

Memcached is a general-purpose distributed memory-caching system: cache data from db.

Asynchronous processing: user doesn't need to wait the job to finish, and continue to do the next task.
   for example: upload video/photo
   message queue: kafka
   
Rehashing problem: 
  most keys are redistributed, not just the ones originally stored in the offline server. 

Consisteng hashing: M hashed key space, and N servers. 
A partition is the hash space between adjacent servers.

1. Create a circular number line(from 0 to M-1).
2. Map the server based on IP or name onto the ring. 
3. Move the hash keys in the clockwise direction until the first server was found. 

  virtual nodes: each server is represented by multiple virtual nodes on the ring. 
	Each server is responsible for multiple partitions. 
	As the number of virtual nodes increases, the distribution of keys becomes more balanced. 
   
