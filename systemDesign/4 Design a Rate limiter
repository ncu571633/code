Control the rate of traffic sent by a client or a service.
Benefit:
  Prevent DoS attack
  Reduce cost

Clarify question:
  client side/server side (client request can easily be forged by malicious actors, may not have control over the client implementation)
  Scale

Requirements
  low latency
  distributed rate limiting
  exception handling

Cloud microservice (API gateway)

Where should the rater limiter be implemented: server side or in a gateway?
  1. Evaluate tech stack: make sure the programming language is efficient to implement rate limiting on the server side.
  2. fits business needs: server side, full control of the algorighm.
  3. already used microservice architecture, included an API gateway for example IP whitelisting.

Algorithm
  1. token bucket
    Token bucket is a container that has pre-defined capacity. 
    Tokens are put in the bucket at preset rates periodically.
    Once the bucket is full, no more tokens are added. 
    Each request consumes one token: enough token in the bucket, take one token out; on enough token, the request is dropped.

    Two parameters: bucket size, refill rate.

  2. Leaking bucket
    queue: request arrives -> queue is not full -> add the request to the queue
                           -> queue is full -> drop the request
           pull request from the queue and processed at regular intervals.
    pros: memory efficient
    cons: hard to tune parameter: queue size

  3. fixed window counter
    divides the timeline into fix-sized time windows and assign a counter for each window
    each request increments the counter by one
    once the counter reaches the pre-defined threshold, new requests are dropped until a new time window starts.
    for example: 1 second and the system allows a maximum of 3 requests per second. 
      in each second window, if more than 3 requests are received, extra requests are dropped. 
    cons: spike in traffic at the edges of a window could cause more requests than the allowed quota to go through.

High level architecture: 
  need a counter to keep track of how many requests are sent from the same user, IP address. 
  In memory cache to store the counter (Redis)
    Redis command: INCR, EXPIRE

  client -> rate limiter middleware -> Redis
                                    -> (success) API servers
                                    -> cached rules <- workers -> rules on the disk

Rate limiting Rules
  a configuration file and saved on disk.

Exceeding the rate limit:
  return HTTP response code 429 to the client.
  Depends on the use case, we may enqueue the request to be processed later. 

Race condition:
  sorted sets data structure in Redis

Synchronization issue:
  To support millions of users, one rate limiter server is not enough. 
  Use multiple rate limiter servers, synchronization is required. 
  Web tier is stateless, clients can send requests to different rate limiter. Client 1 -> rate limiter 1, also client 1 -> rate limiter 2. (Share counter)
  ONe possible solution is to use sticky sessions that allow a client to send traffic to the same rate limiter (not flexible/scalable)
  Use redis: centralized data store. 
