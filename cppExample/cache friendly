cache friendly code
  Data is always retrieved through the memory hierarchy: CPU register, L1/L2/L3 cache, memory
  modern CPUs spend around 85% of die on caches and up to 99% for storing/moving data!

************************************************************************************************************

instruction cache would include cache lines fetched from memory for execution. 
The data cache would include cache lines fetched from memory for loading into a register as data.

************************************************************************************************************

memory:

high address

       | stack ->     <- heap | bbs(uninitialized data) | data segment(initialized data) | text

data segment: initialized static variables: global variables and static local variables
    size is determined by the size of the values in the program's source code, and does not change at run time. 
bss: block starting symbol: read-only data segment: static const
code segment(text segment, or text): executable instructions

************************************************************************************************************
    Use smaller data types
    Organize data to avoid alignment holes (sorting your struct members by decreasing size is one way)
    avoid algorithms and datastructures that exhibit irregular access patterns, and favor linear datastructures.
    Beware of the standard dynamic memory allocator, which may introduce holes and spread your data around in memory as it warms up.
    Make sure all adjacent data is actually used in the hot loops. Otherwise, consider breaking up data structures into hot and cold components, 
so that the hot loops use hot data.

************************************************************************************************************
clock cycles
  Data from your memory gets brought over to the CPU in little chunks (called 'cache lines'), typically 64 bytes.
  If you have 4-byte integers, that means you're geting 16 consecutive integers in a neat little bundle

************************************************************************************************************
Temporal locality: it is likely that the same location is accessed again in the near future
Spatial locality: placing related data close to each other

************************************************************************************************************
Use appropriate c++ containers
  Elements of a std::vector are stored in contiguous memory: more cache-friendly than std::list

************************************************************************************************************
Know and exploit the implicit structure of data
  matrix:
    column major: fortran, matlab
    row major: c/c++    static int x[4000][4000];
    When fetching a certain element of a matrix from memory, elements near it will be fetched as well and stored in a cache line. 
    If the ordering is exploited, this will result in fewer memory accesses (because the next few values which are needed for subsequent computations are already in a cache line).

************************************************************************************************************

Avoid unpredictable branches
            for (int c = 0; c < arraySize; ++c)
            {   // Primary loop.
                if (data[c] >= 128)
                    sum += data[c];
            }
            
            Branch Prediction.
            if the array is sorted, it is faster. 
            eliminates the branch

************************************************************************************************************
Avoid virtual functions
  cache misses
  Virtual functions can induce cache misses during look up, but this only happens if the specific function is not called often
  This can cause a large pipeline bubble as the processor cannot fetch any instructions until the indirect jump 
  (the call through the function pointer) has retired and a new instruction pointer computed.

************************************************************************************************************
Line Sharing - False sharing
  cache line shared by two threads.
  One elements in this cache line got modified by one thread and mark this line as invalid. 
  Other processors accessing a different element in the same line see the line marked as invalid. 
  They are forced to fetch a more recent copy of the line from memory or elsewhere, even though the element accessed has not been modified. 
  This is because cache coherency is maintained on a cache-line basis, and not for individual elements


 10. Write Cache-Friendly Code

10.1. Place related data close in memory to allow efficient caching – the principle of locality

10.2. Understand how cache lines work

10.3. Use appropriate data structures

10.4. Avoid unpredictable branches

10.5. Avoid virtual functions

10.6. Avoid false sharing problem

10.7. When a context switch happens the processor involved is likely to lose the data in its caches

10.8. Try to have a regular access pattern that will let the hardware prefetcher work efficiently

10.9. Start addressing what is called temporal locality

10.10. Merge loops that touch the same data (loop fusion), and employ rewriting techniques known as tiling or blocking

** Following those rules will minimize the number of page faults (caused by thrashing) – latency killer.

                                            

                                           C++ Techniques                                                                   

01. Make the common case fast and the rare case correct.

02. Code for correctness first, then optimize!

03. Optimize - People I know who write very efficient code say they spend at least twice as long optimizing code as they spend writing code.

04. Jumps/branches are expensive. Minimize their use whenever possible.

05. Think about the order of array indices.

06. Think about instruction-level-parallelism.

07. Avoid/reduce the number of local variables.

08. Reduce the number of function parameters.

09. Pass structures by reference, not by value.

10. If you do not need a return value from a function, do not define one.

11. Try to avoid casting where possible.

12. Be very careful when declaring C++ object variables.

13. Make default class constructors as lightweight as possible.

14. Use shift operations >> and << instead of integer multiplication and division, where possible.

15. Be careful using table-lookup functions.

16. For most classes, use the operators += , -= , *= , and /= , instead of the operators + , - , * , and / .

17. For basic data types, use the operators + , - , * , and / instead of the operators += , -= , *= , and /= .

18. Delay declaring local variables.

19. For objects, use the prefix operator (++obj) instead of the postfix operator (obj++).

20. Be careful using templates.

21. Avoid dynamic memory allocation during computation.

22. Find and utilize information about your system’s memory cache.

23. Avoid unnecessary data initialization.

24. Try to early loop termination and early function returns.

25. Simplify your equations on paper!

26. The difference between math on integers, fixed points, 32-bit floats, and 64-bit doubles is not as big as you might think.

27. Consider ways of rephrasing your math to eliminate expensive operations. 


Heap fragmentation is a state in which available memory is broken into small, noncontiguous blocks.
 When a heap is fragmented, memory allocation can fail even when the total available memory in the heap is enough to satisfy a request, because no single block of memory is large enough.


Consequence 
1: Unreliable program
We have a lot of free memory, but you can only allocate small blocks. 
If your program needs a bigger block, it will not get it and will stop working.

2: Degraded performance
slower because the memory allocator takes more time to find the best hole in the free list, the so-called “best-fit.”


Strategy:
Short string optimization: only use heap for long string
Heap compacting: not C++, since pointers would become invalid. 
Memory pool: allocate a large block. 
fragmentation-free custom allocator, like object pool

Avoid heap (in particular, avoid String)
In many cases, we can avoid dynamic allocation. Instead of allocating objects in the heap, we place them in the stack or in the globals. By design, these two areas are not fragmented.

For example, we could replace all String objects with plain old char[]. Not only we would reduce the fragmentation, but we would also create a smaller and faster executable.


Allocate small, fixed-sized objects on the stack to avoid the cost of an unnecessary heap allocation and free
repeated allocations of the same size don’t cause fragmentation; so, we could keep our objects in the heap but always use the same size.

For example, if we have a string that can have between 10 and 100 characters, we could always reserve 100 characters:
myString.reserve(100);
