A process is an instance of a computer program that is being executed
Thread: A thread is a light-weight process.
1. processes are typically independent, while threads exist as subsets of a process
processes carry considerably more state information than threads, whereas multiple threads within a process share process state as well as memory and other resources
processes have separate address spaces, whereas threads share their address space
2. processes interact only through system-provided inter-process communication mechanisms files, pipes, or sockets
two threads in a task can communicate through memory. 
3. context switching between threads in the same process is typically faster than context switching between processes.
When one thread stops executing and another starts, we call that a thread context switch

Starvation
The reason that thread 1 never runs is that thread 0 never voluntarily gives up the CPU

Preemption This means that if one user-level thread is running, then at some point the operating system will interrupt it
and run another user-level thread
pthread_attr_setscope(attr+i, PTHREAD_SCOPE_SYSTEM)

A race condition occurs when 2 or more threads are able to access shared data and they try to change it at the same time. 

In our race program, we can fix the race condition by enforcing that no thread can be interrupted by another thread when it is modifying and printing s.
This can be done with a mutex, sometimes called a ``lock'' or sometimes a ``binary semaphore.'' 

If you are performing a very time consuming operation (such as writing to a socket or file) while holding the mutex, 
then you should consider the use of buffering so that you can move the time consuming operation out of the code that holds the mutex. 

A monitor is a data structure which a thread can "enter" and "exit". Only one thread may be in the monitor at a time. 
This is just like a mutex, and in pthreads, there is no entity called a "monitor". You just use a mutex. 

Condition variables allow you to do more sophisticated things with monitors. 
A condition variable must be associated with a specific monitor. 

Flynn's classification
Single instruction single data: sequential
Single instruction multiple data: array processor: data parallelism 
Multiple instruction single data: pipe line
Thread Models Description
Delegation model     Thread pool
A central thread (boss) creates the threads (workers), assigning each worker a task. 
Each worker is assigned a task by the boss thread. The boss thread may wait until each thread completes that task.
Peer-to-peer model
All the threads have an equal working status. Threads are called peer threads.
A peer thread creates all the threads needed to perform the tasks but performs no delegation responsibilities. 
The peer threads can process requests from a single input stream shared by all the threads or each thread may have its own input stream.
Pipeline
An assembly-line approach to processing a stream of input in stages. 
Each stage is a thread that performs work on a unit of input. When the unit of input has been through all the stages, 
then the processing of the input has been completed.
Producer–consumer model
A producer thread produces data to be consumed by the consumer thread. The data is stored in a block of memory 
shared by the producer and consumer threads.

A deadlock occurs when there is a circular chain of threads or processes which each hold a locked resource and 
are trying to lock a resource held by the next element in the chain. 
For example, two threads that hold respectively lock A and lock B, and are both trying to acquire the other lock.

we can prevent deadlock if we assign an order to our locks and require that locks always be acquired in order. 

Mutual exclusion condition: a resource that cannot be used by more than one process at a time
Hold and wait condition: processes already holding resources may request new resources
No preemption condition: No resource can be forcibly removed from a process holding it, resources can be released only by the explicit action of the process
Circular wait condition: two or more processes form a circular chain where each process waits for a resource that the next process in the chain holds

Here, the important point is that when one process is executing shared modifiable data in its critical section,
no other process is to be allowed to execute in its critical section

mutual exclusion (short for Mutex) refers to the requirement of ensuring that no two processes or threads 
(henceforth referred to only as processes) are in their critical section at the same time. 

Monitor
a monitor is an object or module intended to be used safely by more than one thread
When locks and condition variables are used together like this, the result is called a monitor : 
A collection of procedures manipulating a shared data structure. 
One lock that must be held whenever accessing the shared data (typically each procedure acquires the lock at the very beginning and releases 
the lock before returning). 
One or more condition variables used for waiting. 

synchronization
Thread synchronization is the application of particular mechanisms to ensure that two concurrently-executing threads or processes do not execute 
specific portions of a program at the same time. If one thread has begun to execute a serialized portion of the program, 
any other thread trying to execute this portion must wait until the first thread finishes. 

Thread-safe
A piece of code is thread-safe if it only manipulates shared data structures in a manner that guarantees safe execution by multiple threads at the same time
avoiding race condition

************************************************
Threading primitives
Mutex (mutual exclusion object): ensures that only one thread can access a shared resource at a time, providing exclusive access
    Protects critical sections to prevent race conditions 
Semaphores: allows multiple threads to access a shared resource concurrently, within a specified limit
    Mutex works upon the locking mechanism, Semaphore uses signaling mechanism(wait/signal).
    No Ownership: Any thread can increment or decrement the semaphore
    Wait (P operation): This operation checks the semaphore's value. If the value is greater than 0, the process is allowed to continue, 
and the semaphore's value is decremented by 1. If the value is 0, the process is blocked (waits) until the semaphore value becomes greater 
than 0.
    Signal (V operation): After a process is done using the shared resource, it performs the signal operation. This increments the 
semaphore's value by 1, potentially unblocking other waiting processes and allowing them to access the resource.

Condition variable: used to wait for a particular condition to become true 
    Always used with a mutex to protect the condition check
    wait/signal/broadcast

std::barrier: It operates as a barrier that all threads must reach before the program can continue
    The std::barrier class sets itself apart from other synchronization primitives by allowing users
    to specify a callback function that executes after the barrier is lifted. 
    This callback function can perform additional tasks that must be completed after the barrier has been crossed
    When the completion step finishes, the expected count is reset to the value specified 
    at construction less the number of calls to arrive_and_drop since, and the next barrier phase begins
    #include <barrier>
    #include <iostream>
    #include <thread>
    std::barrier my_barrier{ 3, [](){ std:: cout<<"... done\n";} };
    void my_function()
    {
        my_barrier.arrive_and_wait();
    }
    int main()
    {
        // creating threads
        std::thread t1(my_function);
        std::thread t2(my_function);
        std::thread t3(my_function);
        t1.join();
        t2.join();
        t3.join();
        std::cout << "All threads have finished\n";
        return 0;
    }

std::latch: a downward counter of type std::ptrdiff_t 
The value of the counter is initialized on creation. Threads may block on the latch until the counter is decremented to zero
There is no possibility to increase or reset the counter, which makes the latch a single-use barrier. 








************************************************
std::lock_guard:
    exclusive ownership of a mutex for a scoped duration
    The lock is acquired upon construction and automatically released when the std::lock_guard object goes out of scope
    It does not support manual unlocking or relocking of the mutex.

std::unique_lock
    supports both exclusive ownership and shared ownership of a mutex
    Allows manual unlocking, relocking, and transferring of ownership between different scopes or threads.
    Provides advanced functionalities like Deferred locking, Timeout locks, condition variables, Transfer of ownership (being moveable), 
and deadlock avoidance.

std::scoped_lock is for the simple case of wanting to lock some number of mutex objects in a deadlock-free way. 
    Locking a single mutex is just a special case of locking multiple ones

std::shared_lock
    designed for shared ownership of a mutex, enabling multiple readers.
    Allows multiple threads to acquire the lock concurrently for shared access.
    Similar to std::lock_guard, it does not support manual unlocking or relocking.

Difference:
lock_guard and unique_lock are pretty much the same thing; lock_guard is a restricted version with a limited interface.
unique_lock calls lock() on the mutex. shared_lock calls shared_lock().

lock_guard/scoped_lock/unique_lock are for the exclusive ownership, shard_lock is for shared ownership.
lock_guard if you need to lock exactly 1 mutex for an entire scope.
scoped_lock if you need to lock a number of mutexes that is not exactly 1.
unique_lock if you need to unlock within the scope of the block (which includes use with a condition_variable).



In computing, a computer program or subroutine is called reentrant if it can be interrupted in the middle of its execution and then safely called again 
("re-entered") before its previous invocations complete execution. The interruption could be caused by an internal action such as a jump or call, or by an external action such as a hardware interrupt or signal.


The pthread_join() function waits for the thread specified by thread to terminate.
Preemptive scheduling: The preemptive scheduling is prioritized. The highest priority process should always be the process that is currently utilized.

Non-Preemptive scheduling: When a process enters the state of running, the state of that process is not deleted from the scheduler until it finishes 
its service time.

Why do you need a while loop while waiting for a condition variable

Consider:
You put a job on a queue.
You signal the condition variable, waking thread A.
You put a job on a queue.
You signal the condition variable, waking thread B.
Thread A gets scheduled, does the first job.
Thread A finds the queue non-empty and does the second job.
Thread B gets scheduled, having been woken, but finds the queue still empty.



************************************************
False sharing
Imagine two threads accessing different variables within the same cache line.
If one thread writes to its variable, the entire cache line is marked as invalid in other processors' caches
How to avoid:
1. use thread private data
2. Add padding (unused bytes) to data structures or arrays so that variables accessed by different threads are placed on separate cache lines

Since caches operate at the granularity of cache lines (typically 64 bytes), 
struct ThreadData {
    int value;
    char pad[60]; // Padding to ensure each struct is 64 bytes
};

************************************************
Spurious wakeup: when a thread is waiting on a condition variable, but it wakes up even though it was not explicitly signaled, 
interrupted, or timed out, and the condition it was waiting for is still unsatisfied

Why: 
Race Conditions: When multiple threads are waiting on a condition variable and a signal or broadcast occurs, 
all may be woken, but only one will find the condition satisfied; the others experience spurious wakeups

Always use a loop to check the condition after waking from a condition variable wait.

std::unique_lock<std::mutex> lock(mtx);
while (!condition) {
    cv.wait(lock);
}
// Proceed only if condition is true

Or, using the C++11 lambda predicate:
cv.wait(lock, []{ return data_ready; }); // This form internally loops until the predicate is true

************************************************
Spinning lock vs sleep lock
Spin Lock(Busy-wait)： thread attempting to acquire the lock repeatedly checks (spins) until the lock becomes available.
Sleep Lock：A sleep lock (sometimes implemented as a mutex or semaphore) puts the waiting thread to 
sleep if the lock is not available.

CPU Usage:  high  vs low
Overhead:	Low for short waits	vs High (context switch)
Best Use Case:	Short critical sections	vs. Long critical sections

************************************************
std::jthread C++20
Automatic Joining: in destructor
Stop Request Mechanism: std::stop_token and std::stop_source
Compared to std::thread: safter due to automatic joining

std::thread
Not joined, but goes out of scope.
When a thread object is destroyed, its destructor checks if the thread is still joinable (i.e., it has not been joined or detached).
If the thread is joinable at destruction, the destructor calls std::terminate
This is a safety feature: automatically joining in the destructor could cause deadlocks or unexpected blocking,
while detaching could lead to dangerous situations where threads outlive the data they reference.

Joined:	Safe, thread cleaned up
Detached:	Safe, thread runs independently
Joinable (not joined/detached):	std::terminate called, program aborts

************************************************
#include <iostream>
#include <mutex>
#include <thread>

int i = 0;
std::mutex m;

void fun(int n)
{
    std::scoped_lock<std::mutex> lck(m);
    i+=n;
}

int main()
{
    {
        std::jthread t1(fun, 10);
        std::jthread t2(fun, 10);
        
        // thread join here when destroy
    }
    
    std::cout<<i;

    return 0;
}

